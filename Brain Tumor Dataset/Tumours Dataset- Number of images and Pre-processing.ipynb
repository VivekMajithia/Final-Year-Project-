{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Data'\n",
        "total_images = 0\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            total_images += 1\n",
        "print(\"Total number of images in the dataset:\", total_images)\n",
        "\n",
        "#Number of images currently in the dataset for Tumours\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVr-JmmGOyHm",
        "outputId": "1c32964e-d792-4d21-94ae-ebda692dca52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images in the dataset: 3096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "zip_file_path = '/content/drive/MyDrive/Colab Notebooks/archive (1).zip'\n",
        "extracted_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "dataset_path = os.path.join(extracted_dir, 'Data')\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=42\n",
        ")\n",
        "# Here we are splitting the dataset into training and validation sets\n",
        "validation_percentage = 0.2\n",
        "num_validation_examples = int(validation_percentage * info.splits['train'].num_examples)\n",
        "val_ds = train_ds.take(num_validation_examples)\n",
        "train_ds = train_ds.skip(num_validation_examples)\n"
      ],
      "metadata": {
        "id": "0vTrve3Acwx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}