{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Data'\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "train_dataset = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "val_dataset = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "num_classes = len(train_dataset.class_indices)\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "class TrainingAccuracyCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Epoch {epoch + 1}/{self.params['epochs']} - Training Accuracy: {logs['accuracy']:.4f}\")\n",
        "\n",
        "accuracy_callback = TrainingAccuracyCallback()\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[accuracy_callback])\n",
        "final_training_accuracy = history.history['accuracy'][-1]\n",
        "print(\"Training accuracy:\", final_training_accuracy)\n",
        "#Training accuracy for Tumour dataset to ensure it is ready to classify images prior to checking unseen data from the test set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAHLmE-BllVo",
        "outputId": "83a15938-c766-438b-92bf-7bb340914204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3096 images belonging to 4 classes.\n",
            "Found 0 images belonging to 4 classes.\n",
            "Epoch 1/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.4607 - accuracy: 0.3453Epoch 1/10 - Training Accuracy: 0.3453\n",
            "97/97 [==============================] - 352s 4s/step - loss: 1.4607 - accuracy: 0.3453\n",
            "Epoch 2/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.5252Epoch 2/10 - Training Accuracy: 0.5252\n",
            "97/97 [==============================] - 54s 553ms/step - loss: 1.0904 - accuracy: 0.5252\n",
            "Epoch 3/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.9422 - accuracy: 0.5898Epoch 3/10 - Training Accuracy: 0.5898\n",
            "97/97 [==============================] - 54s 558ms/step - loss: 0.9422 - accuracy: 0.5898\n",
            "Epoch 4/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.8041 - accuracy: 0.6589Epoch 4/10 - Training Accuracy: 0.6589\n",
            "97/97 [==============================] - 53s 544ms/step - loss: 0.8041 - accuracy: 0.6589\n",
            "Epoch 5/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.7389 - accuracy: 0.6928Epoch 5/10 - Training Accuracy: 0.6928\n",
            "97/97 [==============================] - 52s 541ms/step - loss: 0.7389 - accuracy: 0.6928\n",
            "Epoch 6/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6966 - accuracy: 0.7074Epoch 6/10 - Training Accuracy: 0.7074\n",
            "97/97 [==============================] - 51s 521ms/step - loss: 0.6966 - accuracy: 0.7074\n",
            "Epoch 7/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.7516Epoch 7/10 - Training Accuracy: 0.7516\n",
            "97/97 [==============================] - 52s 541ms/step - loss: 0.6017 - accuracy: 0.7516\n",
            "Epoch 8/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.7894Epoch 8/10 - Training Accuracy: 0.7894\n",
            "97/97 [==============================] - 51s 525ms/step - loss: 0.5371 - accuracy: 0.7894\n",
            "Epoch 9/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.7952Epoch 9/10 - Training Accuracy: 0.7952\n",
            "97/97 [==============================] - 53s 546ms/step - loss: 0.5048 - accuracy: 0.7952\n",
            "Epoch 10/10\n",
            "97/97 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.8169Epoch 10/10 - Training Accuracy: 0.8169\n",
            "97/97 [==============================] - 50s 520ms/step - loss: 0.4750 - accuracy: 0.8169\n",
            "Training accuracy: 0.8168604373931885\n"
          ]
        }
      ]
    }
  ]
}